[H-01] Cannot actually submit evidence
Submitted by jmak
Impact
TheSubmitBadSignatureEvidenceis not actually registered in the handler and hence no one can actually submit this message, rendering the message useless. This harms the security model of Gravity since validators have no disincentive to attempt to collude and take over the bridge.
Proof of Concept
TheSubmitBadSignatureEvidencehandler is omitted frommodule/x/gravity/handler.go
Tools Used
Visual inspection
Recommended Mitigation Steps
Handle theMsgSubmitBadSignatureEvidenceinmodule/x/gravity/handler.go
jkilpatr (Althea) confirmed and patched:
This was resolved herehttps://github.com/althea-net/cosmos-gravity-bridge/commit/ad6bd78d4c968c3eef5a8ab7a38b42cd3269d186This is a valid bug considering this fix is not included in the code hash up for review.
[H-02] Freeze Bridge via Non-UTF8 Token Name/Symbol/Denom
Submitted by nascent
Manual insertion of non-utf8 characters in a token name will break parsing of logs and will always result in the oracle getting in a loop of failing and early returning an error. The fix is non-trivial and likely requires significant redesign.
Proof of Concept
Note thec0in the last argument of the call data (invalid UTF8).
It can be triggered with:
datamemorybytes=hex"f7955637000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000c000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000012000000000000000000000000000000000000000000000000000000000000000461746f6d0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000046e616d6500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000673796d626fc00000000000000000000000000000000000000000000000000000";gravity.call(data);
The log output is as follows:
ERC20DeployedEvent("atom","name", ❮utf8decodefailed❯:0x73796d626fc0,18,2)
Which hitsthis code path:
letsymbol =String::from_utf8(input.data[index_start..index_end].to_vec());trace!("Symbol {:?}", symbol);ifsymbol.is_err() {returnErr(GravityError::InvalidEventLogError(format!("{:?} is not valid utf8, probably incorrect parsing",symbol)));}
And would cause an early returnhere:
leterc20_deploys = Erc20DeployedEvent::from_logs(&deploys)?;
Never updating last checked block and therefore, this will freeze the bridge by disallowing any attestations to take place. This is an extremely low cost way to bring down the network.
Recommendation
This is a hard one. Re-syncing is permanently borked because, on the Go side, there is seemingly no way to ever process the event nonce because protobufs do not handle non-utf8 strings. The validator would report they need event nonceNfrom the orchestrator, but they can never parse the eventN. Seemingly, validators & orchestrators would have to know to ignore that specific event nonce. But it is a permissionless function, so it can be used to effectively permanently stop attestations & the bridge until a newGravity.solis deployed.
One potential fix is to check in the solidity contract if the name contains valid utf8 strings for denom, symbol and name. This likely will be expensive though. Alternatively, you could require that validators sign ERC20 creation requests and perform checks before the transaction is sent.
jkilpatr (Althea) confirmed:
This is a valid and well considered bug.I do disagree about the difficulty of the fix though, if we fail to parse the token name as utf8 we can just encode the bytes themselves in hex and pass that along. The result will be perfectly valid if a little unergonomic.
albertchon (judge) commented:
Clever, great catch
[H-03] Freeze The Bridge Via Large ERC20 Names/Symbols/Denoms
Submitted by nascent
Ethereum Oracles watch for events on theGravity.solcontract on the Ethereum blockchain. This is performed in thecheck_for_eventsfunction, and run in theeth_oracle_main_loop.
In this function, there isthe following code snippet:
leterc20_deployed = web3.check_for_events(starting_block.clone(),Some(latest_block.clone()),vec![gravity_contract_address],vec![ERC20_DEPLOYED_EVENT_SIG],).await;
This snippet leverages theweb30library to check for events from thestarting_blockto thelatest_block. Inside theweb30library this nets out to calling:
pubasyncfneth_get_logs(&self, new_filter: NewFilter) ->Result<Vec<Log>, Web3Error> {self.jsonrpc_client.request_method("eth_getLogs",vec![new_filter],self.timeout,Some(10_000_000),).await}
The10_000_000specifies the maximum size of the return in bytes and returns an error if the return is larger:
letres: Response<R> =matchres.json().limit(limit).await{Ok(val) => val,Err(e) =>returnErr(Web3Error::BadResponse(format!("Web3 Error {}", e))),};
This can be triggered at will and keep the loop in a perpetual state of returning theGravityError::EthereumRestError(Web3Error::BadResponse( "Failed to get logs!".to_string()))error. To force the node into this state, you just have to deploy ERC20s generated by thepublic function inGravity.sol:
functiondeployERC20(stringmemory_cosmosDenom,stringmemory_name,stringmemory_symbol,uint8_decimals)public{// Deploy an ERC20 with entire supply granted to Gravity.solCosmosERC20erc20=newCosmosERC20(address(this),_name,_symbol,_decimals);// Fire an event to let the Cosmos module knowstate_lastEventNonce=state_lastEventNonce.add(1);emitERC20DeployedEvent(_cosmosDenom,address(erc20),_name,_symbol,_decimals,state_lastEventNonce);}
And specify a large string as the denom, name, or symbol.
If an attacker uses the denom as the attack vector, they save significant gas costing just 256 per additional 32 bytes. For other cases, to avoid gas overhead, you can have the string be mostly 0s resulting in just 584 gas per additional 32 bytes. This leaves it feasible to surpass the 10mb response data in the 6 block buffer. This would throw every ethereum oracle into a state of perpetual errors and all would fall out of sync with the ethereum blockchain. This would result in the batches, logic calls, deposits, ERC20 creations, andvalsetupdates to never receive attestations from other validators because their ethereum oracles would be down; the bridge would be frozen and remain frozen until the bug is fixed due toget_last_checked_block.
This will freeze the bridge by disallowing attestations to take place.
This requires a patch to reenable the bridge.
Recommendation
Handle the error more concretely and check if you got a byte limit error. If you did, chunk the search size into 2 and try again. Repeat as necessary, and combine the results.
Additionally, you could require that validators sign ERC20 creation requests.
jkilpatr (Althea) confirmed:
Excellent bug report.I just ran into the buffer limit issue this morning with an Ethereum block. I agree handling this error correctly is essential to long term reliability.
albertchon (judge) commented:
Nice :)
[H-04] Large Validator Sets/Rapid Validator Set Updates May Freeze the Bridge or Relayers
Submitted by nascent
In a similar vein to “Freeze The Bridge Via Large ERC20 Names/Symbols/Denoms”, a sufficiently large validator set or sufficiently rapid validator update, could cause both theeth_oracle_main_loopandrelayer_main_loopto fall into a state of perpetual errors. Infind_latest_valset,we call:
letmutall_valset_events = web3.check_for_events(end_search.clone(),Some(current_block.clone()),vec![gravity_contract_address],vec![VALSET_UPDATED_EVENT_SIG],).await?;
Which if the validator set is sufficiently large, or sufficiently rapidly updated, continuoussly return an error if the logs in a 5000 (see:const BLOCKS_TO_SEARCH: u128 = 5_000u128;) block range are in excess of 10mb. Cosmos hub says they will be pushing the number of validators up to 300 (currently 125). At 300, each log would produce 19328 bytes of data (4*32+64*300). Given this, there must be below 517 updates per 5000 block range otherwise the node will fall out of sync.
This will freeze the bridge by disallowing attestations to take place.
This requires a patch to reenable the bridge.
Recommendation
Handle the error more concretely and check if you got a byte limit error. If you did, chunk the search size into 2 and try again. Repeat as necessary, and combine the results.
jkilpatr (Althea) confirmed:
This is a solid report with detailed computations to back it up. I appreciate it and will take actions in our web3 library to prevent this exact scenario.
