[H-04] Rewards accumulated can stay constant and often not increment
Submitted by moose-code
rewardsPerToken_.accumulatedcan stay constant whilerewardsPerToken_.lastUpdatedis continually updated, leading to no actual rewards being distributed. I.e. No rewards accumulate.
Line 115,rewardsPerToken_.accumulatedcould stay constant if there are very quick update intervals, a relatively lowrewardsPerToken_.rateand a decent supply of the ERC20 token.
I.e. imagine the token supply is 1 billion tokens (quite a common amount, note even if a supply of only say 1 million tokens this is still relevant). i.e. 1e27 wei.
Line 115 has
1e18*timeSinceLastUpdated*rewardsPerToken_.rate/_totalSupply
timeSinceLastUpdatedcan be crafted to be arbitrarily small by simply transferring or burning tokens, so lets exclude this term (it could be 10 seconds etc). Imagine total supply is 1e27 as mentioned.
Therefore,1e18 * rewardsPerToken_.rate / 1e27, which shows that if therewardsPerToken_.rateis < 1e9, something which is very likely, then the accumulated amount won’t increment, as there are no decimals in solidity and this line of code will evaluate to adding zero. While this is rounded down to zero, critically,rewardsPerToken_.lastUpdated = end;is updated.
The reason I have labelled this as a high risk is the express purpose of this contract is to reward users with tokens, yet a user could potentially quite easily exploit this line to ensure no one ever gets rewards and the accumulated amount never increases.
Given a fairly large token supply, and a relatively low emissions rate is set, that satisfies the above equation, for the entire duration of the rewards period, the user simply sends tokens back and forth every couple seconds (gas limitations, but layer 2), to keep the deltatimeSinceLastUpdatedclose to 1.
This way the accumulated amount will never tick up, but time keeps being counted.
Furthermore, I would say this is high risk as this wouldn’t even need an attacker. Given the transfer function is likely often being called by users,timeSinceLastUpdatedwill naturally be very low anyways.
Even if not so extreme as the above case, Alberto points out that “rounding can eat into the rewards” which is likely to be prevalent in the current scenario and make a big impact over time on the targeted vs actual distribution.
Again, this problem is more likely to occur in naturally liquid tokens where lots of transfer, mint or burn events occur.
As suggested by Alberto, the simplest it to probably not update therewardsPerToken_.lastUpdatedfield ifrewardsPerToken_.accumulateddoes not change. Although this change should be closely scrutinized to see it doesn’t introduce bugs elsewhere.
alcueca (Yield) acknowledged and disagreed with severity:
While the issue exists, it’s not as severe as portrayed, and doesn’t need fixing.There is an error in the assessment, and it is that theraterefers to the rewards amount distributed per second among all token holders. It is not the rewards amount distributed per token per second (that’s dynamically calculated).Also, it needs to be taken into account thatrewardsPerToken.accumulatedis stored scaled up by 1e18, to avoid losing much ground to rounding.
structRewardsPerToken{uint128accumulated;// Accumulated rewards per token for the period, scaled up by 1e18uint32lastUpdated;// Last time the rewards per token accumulator was updateduint96rate;// Wei rewarded per second among all token holders}
One of the largest cap tokens is Dai, with a distribution close to 1e28.
If ERC20Rewards were to distribute 1 cent/second among all token holders (which wouldn’t be very exciting), and block times were of 1 second, the accumulator would still accumulate.accumulator += 1e18 (scaling) * 1 (seconds per block) * 1e16 (Dai wei / second) / 1e28 (Dai total supply)The increase to theaccumulatoris of 1e6, which gives plenty of precision. I would expect a rewards program on Dai holders would be at least 1e6 larger per second.On the other hand,accumulatoris anuint128, which holds amounts of up to 1e38. To overflow it we would need a low cap token (let’s say USDC, with 1e15), and a high distribution (1e12 per second, which is unreal), and we run the program for 3 years, or 1e9, to make it easy.The accumulator at the end of the ten years would be:accumulator = 1e18 (scaling) * 1e9 (seconds) * 1e12 (distribution) / 1e15 (supply) = 1e24Which doesn’t overflow.
ghoul-sol (judge) commented:
I’ll keep high risk as there should be no scenario where the math breaks.